<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="Behaviour cloning with World Models"/>
  <meta property="og:url" content="https://suryadheeshjith.github.io/iris_playground/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Behavior Cloning with World Models</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">
  <link rel="stylesheet" href="static/css/gallery2.css">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Behavior Cloning with World Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Laurent Baeriswyl (lvb243),
              </span>
              <span class="author-block">
                Surya Dheeshjith (sd5313)
              </span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block">Courant, New York University<br>DDRL Project Blog (2024)</span>
            </div>
            <!-- Github link -->
            <span class="link-block">
              <a href="https://github.com/suryadheeshjith/iris_playground" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
            </a>
          </span>
          </div>
        </div>
      </div>
    </div>
  </section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>In this project, we combine behavior cloning and world models, two techniques that are frequently used for their in-environment sample efficiency. 
            Building on IRIS<cr cite='#bib01'><sup>1</sup></cr>, a recent transformer-based world model system, we evaluate our approach using 3 Atari environments of differing difficulty levels
             from the Atari100k<cr cite='#bib02'><sup>2<sup></cr> benchmark. 
             We explore different methods of using behavior cloning and our findings show that IRIS agents trained with behavior cloning outperform the baseline 
             model in all of our environments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


  <!-- Introduction -->
  <section class="section" id="Introduction">
    <div class="container is-max-desktop content">
      <h2 class="title">Introduction</h2>
      <p>
        One key challenge in Reinforcement Learning is its sample inefficiency. Popular on-policy algorithms like REINFORCE or PPO frequently require millions of optimization steps and interactions with an environment, making them a subpar choice for computationally expensive environments or real-world rollouts.
      <br/><br/>
        Recent approaches in model-based RL, like ‘Dreamer-v3’ or ‘IRIS,’ <cr cite='#bib03'><sup>3<sup></cr><cr cite='#bib01'><sup>1<sup></cr> have attempted to tackle this problem by learning a ‘world model,’ a model of the environment that predicts future states of the environment given an action. Such approaches have significantly reduced the number of interactions with the environment, with IRIS using only 100k steps, roughly 2 hours of real gameplay time, on its Atari benchmarks.
      <br/><br/>

      Simultaneously, some research has used behavior cloning (BC), which reframes decision-making as a supervised learning task, to avoid expensive and sample-inefficient reinforcement learning training runs altogether. Behavior cloning is also frequently used to initialize agents for further training, which can improve agents and speed up their training time.
      Combining these approaches leads to multiple potential benefits:
      <br/>
      Firstly, during initial epochs, the world models are typically trained using rollouts from an untrained actor. While this strategy might lead to more exploration, the world model will not be able to train on later parts of gameplay and good actions until the actor starts improving. Using a BC actor may allow us to collect better and more representative trajectories to learn the world model.
      Secondly, while BC is often a fast way to gain a strong initialization, it frequently fails to generalize well to states outside its training dataset. By continuing to train the actor using on-policy learning in the world model, we can make the actor more resilient and perform better.
      <br/><br/>
      We implement and evaluate this combined training using IRIS and three different Atari environments from the Atari100k benchmark. The Atari100k benchmark evaluates agents' performance after 100k in-environment steps, approximately equivalent to 2 hours of real gameplay time.
      <br/><br/>
      In this blog post, we will first outline our experiments learning world models in state space and why we abandoned this approach. Then, we will elaborate on our approach to behavior cloning and how we collected samples. We will then explain how our system is put together end-to-end and what steps were taken to improve on a naive BC initialization. Finally, we will discuss our results and takeaways.

      </p>
    </div>
  </section>
  <section class="section" id="IRIS">
    <div class="container is-max-desktop content">
      <h2 class="title">IRIS</h2>
      <p>
        We built our system around IRIS  because of its relative simplicity of architecture. While Dreamer-v3 uses many complicated tricks and components to train its model, IRIS is primarily built out of three components: an autoencoder, a GPT-like decoder-only transformer, which acts as the world model, and an actor network.
      <br/><br/>

      <div style="text-align: center;">
        <img src="static/images/iris_fig.png" alt="IRIS Architecture" width="80%">
        <br/>
        Autoregressive rollout of image observations in the world model using Encoder E, Decoder D and World model Transformer G. IRIS agent is represented by &pi;. Image taken from <cr cite='#bib01'><sup>1<sup></cr>
      </div>

      <br/>
      As shown in Figure x, the autoencoder outputs a series of latent representations of atari observations and decodes them back to the observation space, which the actor uses to predict actions. The world model takes the latent representations and predicted actions and outputs a predicted latent representation of the future state.

      <br/><br/>
      IRIS spends 50 epochs training the world model and tokenizer using their autoencoder and world model loss, before train the agent using the REINFORCE algorithm with advantage estimation and an entropy bonus on imagined trajectories.
    </p>
    </div>
  </section>
  <section class="section" id="State-space Observations">
    <div class="container is-max-desktop content">
      <h2 class="title">State Space or Pixel Space Observations?</h2>
      <p>
      The original Atari2600 console represented each game’s state using 128 bytes in its RAM, a significantly smaller representation than pixel space observations. As an initial stage in our project, we wanted to investigate the feasibility of learning a world model using only the environment’s RAM output instead of images. By taking this approach, we could potentially cut down on the amount of compute needed and eliminate the autoencoder from IRIS’ architecture, thereby reducing the number of components that need to be trained in tandem.
      <br/><br/>
      Our experiments for this used RAM-only environments and matched the frame stacking logic used in pixel-space IRIS to ensure the model has time-based data. We used expert rollouts in pixel space for our BC data collection and accessed the underlying ALE (Arcade Learning Environment) to collect the RAM observations.
      <br/><br/>
      However, our experimental runs showed that when attempting to learn from RAM only, we fail to learn a good world model and thus fail to learn a good agent.
      <br/><br/>
      Unfortunately, this result seems in line with other empirical findings on using the Atari RAM observations when training traditional RL agents<cr cite='#bib04'><sup>4<sup></cr>: Even though it is tempting to think that a more dense representation will lead to better results and better learning, we found that learning IRIS agents using RAM observations takes significantly more training steps. 

      <strong>Since our initial hypothesis of faster training times was false, we discarded this approach and moved forward with training in pixel space.</strong>
    </p>
    </div>
  </section>
  <section class="section" id="Behavior Cloning">
    <div class="container is-max-desktop content">
      <h2 class="title">Behavior Cloning Data Collection</h2>
      <p>
      To build our system for BC pre-training, we need expert datasets. Since we are working on Atari environments, we used available pre-trained agents. This choice was made for simplicity, but using human expert data or other data sources would be preferred in a scenario without an expert.
      <br/><br/>
      To be able to learn well using behavior cloning, we need to ensure that the training data covers as much of the state space as possible. We additionally need to avoid some common pitfalls, like including contradictory data (different expert actions given the same observation). And, since we intend to use this BC pre-training as initialization for downstream world model training, we need to match the shape of our collected observations to the shape of observations that IRIS uses.
      <br/><br/>
      While collecting the data, we included improvements based on prior work in BC for Atari <cr cite='#bib05'><sup>5<sup></cr>:

      <ol>
        <li>Sticky actions: If we take repeated steps in the environment using a set sticky action probability while continuing to store the expert-predicted actions in our dataset, the expert data will cover a larger amount of the state space.
        </li>
        <li>Initial random steps: By spending a set number of initial steps acting randomly, the expert dataset will reflect a more diverse range of starting positions.
        </li>
      </ol>
      <br/>
      These parameters need to be tuned based on the chosen environment. For example, using a large number of initial random steps on Breakout will simply cause the expert to lose.
      <br/><br/>
      IRIS uses 20 context frames for its world model and actor when working in imagination. At the beginning of the imagined trajectory, the actor’s LSTM hidden states and context are initialized with these 20 frames. After testing different context lengths for our BC training, we found that the actor learns better when given more context, so we collected sufficient information to match the context length provided by IRIS for each target action.
    </p>
    <h3 class="title">Technical Considerations</h3>
      <p>
        Unlike most current on-policy Atari agents, IRIS uses 64 x 64 observations with three color channels, which leads to additional complexity when collecting data through the gym environment.
        <br/><br/>
        We attempted to use experts from stable-baselines zoo, stable-baselines3 zoo, and Atari Agents before finally settling on AtariARI<cr cite='#bib06'><sup>6<sup></cr>'s PPO agents. Throughout this process, we found that most agents were trained using grayscale observations with the default resized dimensions of 84 x 84.
        <br/><br/>
        We overcame this hurdle by running two environments initialized with the same seed in parallel. We used the first environment to provide grayscale observations to the expert and collected color observations for our dataset from the second environment. 
        <br/><br/>
      </p>
    </div>
  </section>

  <section class="section" id="Behavior Cloning Pretraining">
    <div class="container is-max-desktop content">
      <h2 class="title">Pretraining with Behavior Cloning</h2>
      <p>
        When initially pretraining our agent using behavior cloning, we quickly achieve high train accuracies of up to 99%, but with significant overfitting, where evaluation accuracy lags significantly behind. There are several possible reasons for this: Firstly, the environments are simple, but the state space is large, and despite using tricks when collecting the train set, we may not have sufficient coverage for the model to generalize well. Secondly, there may be ambiguous observations, where the agent could take multiple legitimate moves, for which the model learns to memorize the correct answer in the train set.
        <br/><br/>
        <div style="text-align: center;">
          <img src="static/images/bc_fig.png" alt="BC" width="80%">
          <br/>
          BC Pretraining
        </div>
        <br/>
        By looking at the actor's performance in evaluation trajectories, we can see that performance is acceptable but not great, suggesting that we are correctly learning some behaviors but not generalizing well. To boost the performance of our agent and reduce overfitting, we used common regularization techniques like weight decay and commonly used frame augmentations like Random Shifts. Additionally, increasing the dataset size also had a significant positive impact on how well the model generalizes.
        <br/><br/>
        The best setting for pretraining included a BC dataset with 150k samples trained for 6 hours across all environments.
      </p>
    </div>
  </section>

  <section class="section" id="Training IRIS">
    <div class="container is-max-desktop content">
      <h2 class="title">Training IRIS with Pretrained Agent</h2>
      <p>
        We first implemented a naive version of BC initialization for IRIS. After completing our BC agent pre-training, we loaded the pre-trained agent’s weights and launched a standard IRIS training run.
        <br/><br/>
        However, there are additional considerations: Notably, the IRIS actor uses a value function for advantage estimation, which is untrained at the start of IRIS training. The untrained value function could potentially lead to subpar gradient updates and a collapse of the trained agent, similar to what we observed in our homework assignment 2.
        <br/><br/>
        Thus, we experimented with including an additional BC loss term based on our collected dataset as part of the actor’s policy loss. A weighting factor, alpha, determines the balance between the BC and imagination losses. Concretely, the policy loss becomes:
        <br/><br/>

        <div style="text-align: center;">
          <img src="static/images/loss_eq.png" alt="Loss Equation" width="80%">
          <br/>
          Loss Equation
        </div>
        <br/>

        We additionally wanted to evaluate different strategies for decaying the weighting factor. In our experiment, we chose to test a cosine decaying, exponentially decaying, and constant factor for alpha.

        For our final evaluation, we run an unchanged IRIS baseline as well as 4 experiments for each environment: 
        <ol>
          <li>IRIS baseline</li>
          <li>Naive BC initialization</li>
          <li>BC regularization following a cosine decaying pattern</li>
          <li>BC regularization following an exponential decaying pattern</li>
          <li>BC regularization following a constant weighting</li>
        </ol>
        <br/>
        We wanted a selection of environments of different difficulty levels. Based on human-normalized scores with respect to the IRIS agent, we selected Breakout (Easy), Demon Attack (Medium) and Ms Pacman (Hard) as our environments for evaluation. Each experiment was trained for 24 hours on a single RTX8000 GPU, and the final results were computed by taking an average over 64 gameplay trajectories.
      </p>
    </div>
  </section>

  <section class="section" id="Results">
    <div class="container is-max-desktop content">
      <h2 class="title">Results</h2>
      <p> 
        <!-- Table -->
        <table style="text-align: center;">
          <thead>
            <tr>
              <th width="30%">Environment</th>
              <th>No BC</th>
              <th>Naive BC</th>
              <th>BC Regularization (Cosine)</th>
              <th>BC Regularization (Exponential)</th>
              <th>BC Regularization (Constant)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td width="30%">Breakout (20k steps)</td>
              <td>6.062</td>
              <td>2.703</td>
              <td>12.296</td>
              <td>12.031</td>
              <td><strong>12.859</strong></td>
            </tr>
            <tr>
              <td>Breakout (100k steps) <sup>1<sup></td>
              <td>84</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
            </tr>
            <tr>
              <td>Demon Attack (20k steps)</td>
              <td>179.453</td>
              <td><strong>769.921</strong></td>
              <td>283.20</td>
              <td>146.718</td>
              <td>184.921</td>
            </tr>
            <tr>
              <td>Demon Attack (100k steps) <sup>1<sup></td>
              <td>2034</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
            </tr>
            <tr>
              <td>Ms Pacman (20k steps)</td>
              <td>370.156</td>
              <td>445.0</td>
              <td><strong>729.062</strong></td>
              <td>378.437</td>
              <td>544.843</td>
            </tr>
            <tr>
              <td>Ms Pacman (100k steps) <sup>1<sup></td>
              <td>999</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
            </tr>
          </tbody>
        </table>
        <div style="text-align: center;">
          <p>Table 1: .</p>
        </div>
      </p>
    </div>
    <!-- Start of gallery -->
    <div class="container is-max-desktop content">
      <table style="text-align: center;">
        <thead>
          <tr>
            <th width="10%">No BC</th>
            <th width="12.5%">Naive BC</th>
            <th width="13%">Cosine</th>
            <th width="12.5%">Exponential</th>
            <th width="10%">Constant</th>
          </tr>
        </thead>
      </table>
    </div>
    <div class="container">
      <div class="gallery">
          <figure class="gallery__item gallery__item--1">
              <img src="static/images/breakout/breakout_img.gif" alt="Gallery image 1" class="gallery__img">
          </figure>
          <figure class="gallery__item gallery__item--2">
              <img src="static/images/breakout/breakout_naivebcrl.gif" alt="Gallery image 2" class="gallery__img">
          </figure>
          <figure class="gallery__item gallery__item--3">
              <img src="static/images/breakout/breakout_cos.gif" alt="Gallery image 3" class="gallery__img">
          </figure>
          <figure class="gallery__item gallery__item--4">
              <img src="static/images/breakout/breakout_exp.gif" alt="Gallery image 4" class="gallery__img">
          </figure>
          <figure class="gallery__item gallery__item--5">
              <img src="static/images/breakout/breakout_constant.gif" alt="Gallery image 5" class="gallery__img">
          </figure>
      </div>
    </div>
    <br/>
    <div class="columns is-centered has-text-centered">Fig. x Breakout Trajectories</div>
    <br/>
    <div class="container">
      <div class="gallery">
          <figure class="gallery__item gallery__item--1">
              <img src="static/images/demonattack/img.gif" alt="Gallery image 1" class="gallery__img">
          </figure>
          <figure class="gallery__item gallery__item--2">
              <img src="static/images/demonattack/naive.gif" alt="Gallery image 2" class="gallery__img">
          </figure>
          <figure class="gallery__item gallery__item--3">
              <img src="static/images/demonattack/cos.gif" alt="Gallery image 3" class="gallery__img">
          </figure>
          <figure class="gallery__item gallery__item--4">
              <img src="static/images/demonattack/exp.gif" alt="Gallery image 4" class="gallery__img">
          </figure>
          <figure class="gallery__item gallery__item--5">
              <img src="static/images/demonattack/constant.gif" alt="Gallery image 5" class="gallery__img">
          </figure>
      </div>
    </div>
    <br/>
    <div class="columns is-centered has-text-centered">Fig. x Demon Attack Trajectories</div>
    <br/>
    <div class="container">
      <div class="gallery">
          <figure class="gallery__item gallery__item--1">
              <img src="static/images/mspacman/img.gif" alt="Gallery image 1" class="gallery__img">
          </figure>
          <figure class="gallery__item gallery__item--2">
              <img src="static/images/mspacman/naive.gif" alt="Gallery image 2" class="gallery__img">
          </figure>
          <figure class="gallery__item gallery__item--3">
              <img src="static/images/mspacman/cos.gif" alt="Gallery image 3" class="gallery__img">
          </figure>
          <figure class="gallery__item gallery__item--4">
              <img src="static/images/mspacman/exp.gif" alt="Gallery image 4" class="gallery__img">
          </figure>
          <figure class="gallery__item gallery__item--5">
              <img src="static/images/mspacman/constant.gif" alt="Gallery image 5" class="gallery__img">
          </figure>
      </div>
    </div>
    <br/>
    <div class="columns is-centered has-text-centered">Fig. x Ms. Pacman Trajectories</div>
    <br/>
  </section>

  <!-- Conclusion -->
  <section class="section" id="Conclusion">
    <div class="container is-max-desktop content">
      <h2 class="title">Conclusion</h2>
      <p>
        Based on the results, we find that initializing world model training using behavior cloning can have significant positive effects on training. In all three environments, we recorded improvements over the IRIS baseline while maintaining the benefits of both approaches for in-environment sample efficiency. This effect tends to exist for nearly all experiments using BC, though significant variability exists between runs.
        Even though we only train using 20k in-environment steps, roughly equivalent to 24 minutes of real gameplay time and a fraction of the training time (1 day vs seven days), our agent makes significant headway towards the IRIS 100k benchmark on Ms Pacman and starts progressing significantly faster than our own IRIS baseline in Demon Attack.
        However, the training results do not directly indicate if adding a BC term to the policy loss (BC regularization) makes a significant difference, as Naive BC outperforms the regularized versions on Demon Attack, whereas using regularization with a cosine decaying factor performed best on Ms Pacman.
        Furthermore, since we could not train our approach for a length comparable to the IRIS 100k benchmark (7 days), we cannot conclude whether this approach can lead to an overall higher performance or if we will reach a plateau sooner.

        While our results are positive, there are several caveats: Firstly, modern approaches to behavior cloning will use architectures like VAE or BeT to capture multimodal data from experts more adequately. By contrast, most world model approaches will use simpler networks for on-policy learning, meaning there is a limit to how well we can capture expert data.
        World models also tend to use very specific tricks to work. In the case of IRIS, many context frames and resized, full-color observations are used, which sets it apart from most other Atari agents. Since we generated our data, we can match these requirements. Still, in many scenarios where BC is applicable, we need to use whichever data is already available, making using downstream world model training more difficult.
        Finally, each environment has particular quirks, meaning we need to adjust our approach for each environment we are trying to learn.
      </p>
    </div>
  </section>

    <!-- References --> 
    <section class="section" id="References">
      <div class="container is-max-desktop content">
        <h2 class="title">References</h2>
          <ol>
            <li id='bib01'>Micheli, V., Alonso, E., & Fleuret, F. (2022). Transformers are sample-efficient world models. arXiv preprint arXiv:2209.00588.</li>
            <li id='bib02'>Kaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell, R. H., Czechowski, K., ... & Michalewski, H. (2019). Model-based reinforcement learning for atari. arXiv preprint arXiv:1903.00374.</li>
            <li id='bib03'>Hafner, D., Pasukonis, J., Ba, J., & Lillicrap, T. (2023). Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104.</li>
            <li id='bib04'>Sygnowski, J., & Michalewski, H. (2017). Learning from the memory of Atari 2600. In Computer Games: 5th Workshop on Computer Games, CGW 2016, and 5th Workshop on General Intelligence in Game-Playing Agents, GIGA 2016, Held in Conjunction with the 25th International Conference on Artificial Intelligence, IJCAI 2016, New York, USA, July 9-10, 2016, Revised Selected Papers 5 (pp. 71-85). Springer International Publishing.</li>
            <li id='bib05'>Chen, B., Tandon, S., Gorsich, D., Gorodetsky, A., & Veerapaneni, S. (2021, June). Behavioral cloning in atari games using a combined variational autoencoder and predictor model. In 2021 IEEE Congress on Evolutionary Computation (CEC) (pp. 2077-2084). IEEE.</li>
            <li id='bib06'>Anand, A., Racah, E., Ozair, S., Bengio, Y., Côté, M. A., & Hjelm, R. D. (2019). Unsupervised state representation learning in atari. Advances in neural information processing systems, 32.</li>
          </ol>
      </div>
    </section>
<!-- BibTex citation
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@software{dheeshjith2023exploring,
        author = {Dheeshjith, Surya and Baeriswyl, Laurent and Rossi, Matteo},
        month = dec,
        title = {{Exploring 2D UNets and Conditional Diffusion with World Models for Video Generation}},
        url = {https://github.com/suryadheeshjith/Video-Diffusion-Models},
        version = {1.0.0},
        year = {2023}
    }</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->



<!-- Footer -->
<footer class="footer">
  This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
</footer>
<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  <script>
    // Non jQuery handler
    var quotes = document.getElementsByTagName('cr');
    for (var i in quotes) {
      quotes[i].addEventListener('click', function() { window.location = this.getAttribute('cite'); }, false);
    }
  </script>
  </body>
  </html>